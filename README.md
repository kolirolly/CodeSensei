# ğŸ’¡ CodeSensei

**CodeSensei** is an AI-powered coding assistant that helps you write, understand, and explore code using natural language. Whether you're debugging, learning a new language, or need help generating code, CodeSensei is your go-to companion.

---

## ğŸš€ Features

- ğŸ” **Explain Mode** â€“ Understand complex code in simple terms.
- âš¡ **Generate Mode** â€“ Instantly generate code snippets or algorithms.
- ğŸ’¬ **Ask Mode** â€“ Ask questions about programming concepts or syntax.
- ğŸ§  **Auto Mode** â€“ Context-aware suggestions as you type.
- ğŸŒ™ **Light/Dark Themes** â€“ Easily toggle between day and night modes.
- ğŸ§¼ **Clean UI** â€“ Minimal, focused interface for seamless productivity.

---

## ğŸ› ï¸ Tech Stack

| Layer        | Tech                        |
|--------------|-----------------------------|
| Frontend     | React + Vite                |
| Styling      | Pure CSS                    |
| Backend      | FastAPI                     |
| AI Engine    | LLaMA 3.2                   |
| Deployment   | Localhost / Custom Hosting  |

---

## ğŸ§‘â€ğŸ’» Getting Started

### Prerequisites

- Node.js (v16+)
- Python (v3.9+)
- `pip`, `virtualenv`, or `poetry`
- Access to LLaMA 3.2 model (locally or via API)

## ğŸ”§ Frontend Setup (React + Vite)

```bash
# Clone the repo
git clone https://github.com/Shrooooyes/CodeSensei.git
cd CodeSensei
cd frontend
```

### Install frontend dependencies
```bash
npm install
```
### Start the frontend server
```bash
npm run dev
```

## ğŸ§  Backend Setup (FastAPI + LLaMA)

### Navigate to backend folder
```bash
cd backend/
```

### Create virtual environment
```bash
python -m venv venv
source venv/bin/activate   # On Windows use `venv\Scripts\activate`
```

### Install dependencies
```bash
pip install -r requirements.txt
```

### Run FastAPI server
```bash
uvicorn main:app --reload
```
---
## âš ï¸ Make sure LLaMA 3.2 is accessible to your backend (e.g., via HuggingFace Transformers, Ollama, or API endpoint).
---

## ğŸ“ Project Structure
```bash
CodeSensei/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â””â”€â”€ App.jsx
â”‚   â”œâ”€â”€ public/
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ services/llm_handler.py
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
---
## âœ¨ Screenshots
- Light Mode	
![image](https://github.com/user-attachments/assets/0b5783a9-bca7-4f20-84e4-74e30ca7df89)

- Dark Mode
![image](https://github.com/user-attachments/assets/fc240cf5-bad1-44b4-8383-8cef60482d57)

--- 

## ğŸ“„ License

This project is licensed under the MIT License â€“ see the LICENSE file for details.

## ğŸ™Œ Acknowledgements
-  Metaâ€™s LLaMA
-  FastAPI
-  Vite
-   React

---

## Crafted with ğŸ’™ for developers, by developers.
Let me know if you want it adjusted for deployment (like Vercel/Render), Dockerized, or integrated with GitHub Actions for CI/CD.

